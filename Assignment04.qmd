---
title: "Assignment 4"
author: "Penny Jiang"
date: "02/24/2023"
output: html_document
---

## 1 Principal Component Analysis

```{r}
library(datasets)
library(ISLR)
arrest = USArrests
states=row.names(USArrests)
names(USArrests)

```

### Get means and variances of variables
```{r}
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
```

```{r}
# PCA with scaling
# prcomp is a function in R that performs a principal component analysis (PCA) on a given data matrix.
pr.out=prcomp(USArrests, scale=TRUE)
names(pr.out) # Five
```

```{r}
pr.out$center # the centering and scaling used (means)
```

```{r}
pr.out$scale # the matrix of variable loadings (eigenvectors)
```

```{r}
pr.out$rotation
```

```{r}
dim(pr.out$x)
```

```{r}
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)
```

```{r}
pr.out$sdev
```

```{r}
pr.var=pr.out$sdev^2
pr.var
```

```{r}
pve=pr.var/sum(pr.var)
pve
```

```{r}
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
```

```{r}
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
```

### Use factoextra package
```{r}
### install.packages("factoextra")
library(factoextra)
fviz(pr.out, "ind", geom = "auto", mean.point = TRUE, font.family = "Arial")
```
```{r}
fviz_pca_biplot(pr.out, font.family = "Arial", col.var="firebrick1")
```
## 2. K-Means Clustering
#### Computer purchase example: Animated illustration 
#### Adapted from Guru99 tutorial (https://www.guru99.com/r-k-means-clustering.html)
#### Dataset: characteristics of computers purchased.
#### Variables used: RAM size, Harddrive size

```{r}
library(dplyr)
library(ggplot2)
library(RColorBrewer)

computers = read.csv("https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv") 

### Only retain two variables for illustration
rescaled_comp <- computers[4:5] %>%
  mutate(hd_scal = scale(hd),
         ram_scal = scale(ram)) %>%
  select(c(hd_scal, ram_scal))
        
ggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +
  geom_point(pch=20, col = "blue") + theme_bw() +
  labs(x = "Hard drive size (Scaled)", y ="RAM size (Scaled)" ) +
  theme(text = element_text(family="Georgia")) 
```

### install.packages("animation")
```{r}
library(animation)
set.seed(2345)
library(animation)

### Animate the K-mean clustering process, cluster no. = 4
kmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) 
```
### animated K-means output
### Iris example
### Without grouping by species
```{r}
ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + 
  theme_bw() +
  scale_color_manual(values=c("firebrick1","forestgreen","darkblue"))
```

### With grouping by species
```{r}
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + 
  theme_bw() +
  scale_color_manual(values=c("firebrick1","forestgreen","darkblue"))
```

### Check k-means clusters
### Starting with three clusters and 20 initial configurations
```{r}
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
```

```{r}
class(irisCluster$cluster)
```

### Confusion matrix
```{r}
table(irisCluster$cluster, iris$Species)
```

```{r}
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +
  scale_color_manual(values=c("firebrick1","forestgreen","darkblue")) +
  theme_bw()
```

```{r}
actual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + 
  theme_bw() +
  scale_color_manual(values=c("firebrick1","forestgreen","darkblue")) +
  theme(legend.position="bottom") +
  theme(text = element_text(family="Georgia")) 
kmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +
  theme_bw() +
  scale_color_manual(values=c("firebrick1", "darkblue", "forestgreen")) +
  theme(legend.position="bottom") +
  theme(text = element_text(family="Georgia")) 
library(grid)
library(gridExtra)
grid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)
```

## Wine example

#### The wine dataset contains the results of a chemical analysis of wines 
#### grown in a specific area of Italy. Three types of wine are represented in the 
#### 178 samples, with the results of 13 chemical analyses recorded for each sample. 
#### Variables used in this example:
#### Alcohol
#### Malic: Malic acid Ash
##### Source: http://archive.ics.uci.edu/ml/datasets/Wine

### Import wine dataset
```{r}
library(readr)
wine <- read_csv("https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv")
```

### Choose and scale variables
```{r}
wine_subset <- scale(wine[ , c(2:4)])
```

### Create cluster using k-means, k = 3, with 25 initial configurations
```{r}
wine_cluster <- kmeans(wine_subset, centers = 3,
                       iter.max = 10,
                       nstart = 25)
wine_cluster
```

### Create a function to compute and plot total within-cluster sum of square (within-ness)
```{r}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}
```

### plotting values for each cluster starting from 1 to 9
```{r}
wssplot(wine_subset, nc = 9)
```
### Plot results by dimensions
```{r}
wine_cluster$cluster = as.factor(wine_cluster$cluster)
pairs(wine[2:4],
      col = c("firebrick1", "darkblue", "forestgreen")[wine_cluster$cluster],
      pch = c(15:17)[wine_cluster$cluster],
      main = "K-Means Clusters: Wine data")
```

```{r}
table(wine_cluster$cluster)
```

### Use the factoextra package to do more
```{r}
### install.packages("factoextra")
library(factoextra)
fviz_nbclust(wine_subset, kmeans, method = "wss")
```

### Use eclust() procedure to do K-Means
```{r}
wine.km <- eclust(wine_subset, "kmeans", nboot = 2)
```

### Print result
```{r}
wine.km
```

### Optimal number of clusters using gap statistics
```{r}
wine.km$nbclust
```

```{r}
fviz_nbclust(wine_subset, kmeans, method = "gap_stat")
```
### Silhouette plot
```{r}
fviz_silhouette(wine.km)
```

```{r}
fviz_cluster(wine_cluster, data = wine_subset) + 
  theme_bw() +
  theme(text = element_text(family="Georgia")) 
```

```{r}
fviz_cluster(wine_cluster, data = wine_subset, ellipse.type = "norm") + 
  theme_bw() +
  theme(text = element_text(family="Georgia")) 
```

## 3. Hierarchical Clustering

### Dataset: USArrests
```{r}
### install.packages("cluster")
library(cluster)
arrest.hc <- USArrests %>%
    scale() %>%                      
    dist(method = "euclidean") %>%   
    hclust(method = "ward.D2")       

#### Generate dendrogram using factoextra package
fviz_dend(arrest.hc, k = 4, 
          cex = 0.5, 
          k_colors = c("firebrick1","forestgreen","blue", "purple"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE, # Add rectangle (cluster) around groups,
          main = "Cluster Dendrogram: USA Arrest data"
) + theme(text = element_text(family="Georgia"))
```
